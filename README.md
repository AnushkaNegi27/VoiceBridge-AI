# ğŸš€ VoiceBridge-AI

<div align="center">

  **An end-to-end AI system for audio, video, live voice, and YouTube speech translation using open-source multilingual models.**

  [Documentation](https://docs-link.com) <!-- TODO: Add documentation link -->
</div>

---

## ğŸ“– Overview

VoiceBridge-AI is a powerful, full-stack application designed to break down language barriers in multimedia content. It provides a comprehensive solution for real-time or offline speech-to-speech and video translation. By integrating advanced Automatic Speech Recognition (ASR), Neural Machine Translation (NMT), and Text-to-Speech (TTS) capabilities, VoiceBridge-AI processes audio and video input, translates the spoken content into a target language, and synthesizes it into natural-sounding speech, making global communication seamless and accessible. This system is ideal for transcribing lectures, translating interviews, localizing video content, and much more.

---

## âœ¨ Key Features

- ğŸ§ **Audio File Translation**
  - Upload WAV / MP3 files
  - Get translated audio output

- ğŸ¬ **Video Translation**
  - Upload MP4 / MKV videos
  - Extracts speech, translates it, and merges translated audio back

- ğŸ™ï¸ **Live Voice Translation**
  - Record directly from the microphone
  - Receive translated speech output

- ğŸ”— **YouTube Video Translation**
  - Paste YouTube URL
  - Generates translated video with dubbed audio

- ğŸŒ **Multilingual Translation (12+ languages)**
  - English, Hindi, Gujarati, Marathi, Tamil, Telugu, Bengali, Kannada, Malayalam, French, Spanish, German

- ğŸ§  **Open-Source AI Models**
  - Whisper for ASR
  - NLLB-200 for multilingual translation
  - Hybrid TTS (Piper + gTTS)

- âš¡ **Local Inference Support**
  - No mandatory cloud APIs
  - Works fully on local machine

---


## ğŸ§  How It Works

1. User uploads an **audio or video file**
2. Speech is transcribed using **Whisper (ASR)**
3. Transcribed text is translated using **NLLB-200**
4. Translated text is converted into speech
5. Output is returned as:
   - Translated audio file, or
   - Video with translated audio merged

---
## ğŸ”Š Text-to-Speech Strategy

VoiceBridge-AI uses a **hybrid TTS approach**:

- **Piper TTS**  
  - Lightweight, offline, fast inference  
  - Used for supported languages  

- **Google gTTS (fallback)**  
  - Ensures wider language coverage  
  - Used when a language is not supported by Piper  

This hybrid approach was chosen to avoid high GPU/CPU load while still maintaining wide language support.
This approach balances **performance**, **offline capability**, and **language support**.

---

## ğŸŒ Supported Languages

Currently supported languages include:

- English (en)
- Hindi (hi)
- Gujarati (gu)
- Marathi (mr)
- Tamil (ta)
- Telugu (te)
- Bengali (bn)
- Kannada (kn)
- Malayalam (ml)
- French (fr)
- Spanish (es)
- German (de)

The system is designed to be easily extendable to additional languages supported by NLLB-200.

---

## ğŸ–¥ï¸ Screenshots

<!-- TODO: Add actual screenshots of the application's user interface -->
![Screenshot of VoiceBridge-AI Dashboard](path-to-dashboard-screenshot.png)
![Screenshot of Translation in Progress](path-to-translation-screenshot.png)

---

## ğŸ› ï¸ Tech Stack

### Backend & AI
- Python
- FastAPI
- Uvicorn
- Whisper (Open-source ASR)
- Hugging Face Transformers (Translation)
- Piper TTS (Offline Text-to-Speech)
- Google gTTS (Fallback TTS)
- FFmpeg

### Frontend
- React
- Vite
- Tailwind CSS

---

## ğŸ“‚ Project Structure

```text
VoiceBridge-AI/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ routes/
|   |   â””â”€â”€ api.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ asr.py
|   |   â”œâ”€â”€ master_pipeline.py
|   |   â”œâ”€â”€ packaging.py
|   |   â”œâ”€â”€ pipeline.py
â”‚   â”‚   â”œâ”€â”€ translation.py
â”‚   â”‚   â”œâ”€â”€ tts.py
â”‚   â”‚   â”œâ”€â”€ video_pipeline.py
â”‚   â”‚   â””â”€â”€ youtube_video_pipeline.py
|   |
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ audio_preprocessing.ipynb
â”‚   â”œâ”€â”€ whisper_training.ipynb
â”‚   â””â”€â”€ whisper_inference.ipynb
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md

```

---

## âš™ï¸ Installation & Setup

**1ï¸âƒ£ Clone the Repository**
```
git clone https://github.com/AnushkaNegi27/VoiceBridge-AI.git
cd VoiceBridge-AI
```

**2ï¸âƒ£ Backend Setup (Python Virtual Environment)**
```
cd backend

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows
venv\Scripts\activate

# Linux / Mac
source venv/bin/activate

```
Install Dependencies:
```
pip install -r requirements.txt
```
Run backend server:
```
uvicorn app:app --reload
```
Backend will run at:
```
http://127.0.0.1:8000
```

**3ï¸âƒ£ Frontend Setup (React)**
```
cd ../frontend
npm install
npm run dev

```

Frontend will run at:
```
http://localhost:5173
```

---

## ğŸ¯ Project Goals

- Build a real-world AI pipeline (not just a demo)
- Focus on modular and extensible design
- Support both audio and video workflows
- Balance performance, accuracy, and language coverage

---

## ğŸ’¡ Why VoiceBridge-AI?

Unlike simple translation demos, VoiceBridge-AI focuses on:

- Real multimedia workflows (audio, video, YouTube)
- Open-source models instead of paid APIs
- Offline capability
- End-to-end system design, not isolated components

  ---


## ğŸ”® Future Improvements

â±ï¸ Streaming inference for long files

ğŸ“ Subtitle generation

â˜ï¸ Cloud deployment (Docker / CI-CD)

---

## ğŸ‘¤ Author

Anushka Negi

---

## ğŸ“„ License

This project is licensed under the MIT License.



